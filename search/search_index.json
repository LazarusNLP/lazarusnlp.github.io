{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Lazarus NLP!","text":"<p>Lazarus NLP is a collective initiative to revive the dying languages of Indonesia through speech and language technology.</p> <p> </p> Revival of Languages, via OpenJourney"},{"location":"#projects","title":"Projects","text":"NusaBERT: Teaching IndoBERT to be multilingual and multicultural! <p>This project aims to extend the multilingual and multicultural capability of IndoBERT. We expanded the IndoBERT tokenizer on 12 new regional languages of Indonesia, and continued pre-training on a large-scale corpus consisting of the Indonesian language and 12 regional languages of Indonesia. Our models are highly competitive and robust on multilingual and multicultural benchmarks, such as IndoNLU, NusaX, and NusaWrites.</p> IndoT5: T5 Language Models for the Indonesian Language <p>IndoT5 is a T5-based language model trained specifically for the Indonesian language. With just 8 hours of training on a limited budget, we developed a competitive sequence-to-sequence, encoder-decode model capable of fine-tuning tasks such as summarization, chit-chat, and question-answering. Despite the limited training constraints, our model is competitive when evaluated on the IndoNLG (text generation) benchmark.</p> Indonesian Sentence Embedding Models <p>We trained open-source sentence embedding models for Indonesian, enabling applications such as information retrieval (useful for retrieval-augmented generation!) semantic text similarity, and zero-shot text classification. We leverage existing pre-trained Indonesian language models like IndoBERT and state-of-the-art unsupervised techniques and established sentence embedding benchmarks.</p> Indonesian Natural Language Inference Models <p>Open-source lightweight NLI models that are competitive with larger models on IndoNLI benchmark, with significantly less parameters. We applied knowledge distillation methods to small existing pre-trained language models like IndoBERT Lite. These models offer efficient solutions for tasks requiring natural language inference capabilities while minimizing computational resources such as cross-encoder-based semantic search.</p> Many-to-Many Multilingual Translation Models <p>Adapting mT5 to 45 languages of Indonesia, we developed a robust baseline model for multilingual translation for languages of Indonesia. This facilitates further fine-tuning for niche domains and low-resource languages, contributing to greater linguistic inclusivity. Our models are competitive with existing multilingual translation models on the NusaX benchmark.</p>"},{"location":"#languages-of-indonesia","title":"Languages of Indonesia","text":"<p>Although the Indonesian language (Bahasa Indonesia) is the country's national language and lingua franca, the Indonesian people continue to use their region's native/indigenous language on a daily basis.</p> <p>The nation is home to over 700 distinct languages, each with its own characteristics and origins. Some languages share similarities, but each language continues to evolve in its own ways over time and in the regions where the language is predominantly spoken.</p> <p>Wikipedia<sup>1</sup> gives a very nice overview of the spread and usage of several regional languages:</p> Language Number (millions) % of total population Branch Main areas where spoken Javanese 84.3 32.28 Javanese throughout Java Island and several provinces in Sumatra and Kalimantan island. Sundanese 42.0 16.08 Sundanese West Java, Banten, Jakarta Madurese 13.6 5.21 Madurese Madura Island (East Java) Minangkabau 5.5 2.11 Malayic West Sumatra, Riau, Jambi, Bengkulu, Jakarta Buginese 5.0 1.91 South Sulawesi South Sulawesi Palembang Malay 3.9 1.49 Malayic South Sumatra Banjarese 3.5 1.34 Malayic South Kalimantan, East Kalimantan, Central Kalimantan Acehnese 3.5 1.34 Chamic Aceh Balinese 3.3 1.26 Bali-Sasak-Sumbawa Bali Island and Lombok Island Betawi 2.7 1.03 Malay-based creole Jakarta"},{"location":"#tech-languages","title":"Tech \ud83e\udd1d Languages","text":"<p>What is deeply concerning about these languages is that, although they may have millions of active speakers, they might still be prone to becoming endangered<sup>2</sup>. Furthermore, UNESCO classified 137 languages in Indonesia as either vulnerable, definitely endangered, severely endangered, or critically endangered<sup>3</sup>.</p> <p>Despite the richness and diversity of these regional languages, there has very been minimal progress in the application of modern technologies to the many languages of Indonesia. It is only in recent years (~2020) that advancements began to appear, starting with the Indonesian language.</p> <p>Through this project, we hope to see the same successes occur in other languages of Indonesia. In the long run, we also hope that through these technological advancements, we will be able to prevent these languages from becoming endangered and in turn, spark innovative work around these languages.</p>"},{"location":"#members","title":"Members","text":"<p>LazarusNLP is driven with love by:</p> <ol> <li> <p>Wikipedia contributors. (2022, May 20). Languages of Indonesia. In Wikipedia, The Free Encyclopedia. Retrieved 07:55, May 21, 2022, from https://en.wikipedia.org/w/index.php?title=Languages_of_Indonesia&amp;oldid=1088838885.\u00a0\u21a9</p> </li> <li> <p>Abtahian, Maya &amp; Cohn, Abigail. (2014). Can a language with millions of speakers be endangered?. Journal of the Southeast Asian Linguistics Society. 7. 64-75.\u00a0\u21a9</p> </li> <li> <p>Moseley, Christopher, ed. (2010). Atlas of the World\u2019s Languages in Danger. Memory of Peoples (3rd ed.). Paris: UNESCO Publishing. ISBN 978-92-3-104096-2.\u00a0\u21a9</p> </li> </ol>"},{"location":"blogs/accents_and_languages/","title":"Indonesian Accents and Regional Languages","text":"<p>It is widely known that there are multiple accents of the English language. Accent is the part of dialect concerning local pronunciation and varies from region to region. English dialects differ greatly in their pronunciation of open vowels.</p> <p>Likewise, Indonesian (as in the language, Bahasa Indonesia) speakers tend to have certain accents, especially if they originate from regions where a secondary regional language is used on a daily basis. As far as our research goes, there is yet to be an existing study that quantifies the different accents of the Indonesian language. Hence, we aim to explore and get a brief understanding of how Indonesian accents vary and how regional languages influence them.</p> <p>Info</p> <p>This blog post is highly inspired by the discussion posted in italki. It raises the question: \"Do people from different islands in Indonesia have a distinct accent when they speak Indonesian? If they do, what are characteristics of those accents like?\"</p>"},{"location":"blogs/accents_and_languages/#language-phonologies","title":"Language Phonologies","text":"<p>We will only be covering examining regional languages Javanese (<code>jv</code>), Sundanese (<code>su</code>), Toba Batak (<code>bbc</code>) and will be comparing them with Indonesian (<code>id</code>).</p>"},{"location":"blogs/accents_and_languages/#indonesian","title":"Indonesian","text":"<p>Vowel phonemes</p> Front Central Back Close <code>i</code> <code>u</code> Close-Mid <code>e</code> <code>\u0259</code> <code>o</code> Open-Mid <code>\u025b</code> <code>\u0254</code> Open <code>a</code> <p>It is usually understood that there are 6 vowel phonemes in the Indonesian language, particularly <code>a</code>, <code>e</code>, <code>i</code>, <code>o</code>, <code>u</code>, and <code>\u0259</code>. However, newer systems tend to add two more open-mid vowels <code>\u0254</code> and <code>\u025b</code>. For simplicity, we will be ignoring the latter two new phonemes and consider the first 6 as standard.</p> <p>Consonant phonemes</p> Labial Dental/Alveolar Palatal Velar Glottal Nasal <code>m</code> <code>n</code> <code>\u0272</code> <code>\u014b</code> Plosive/Affricate voiceless <code>p</code> <code>t\u032a</code> <code>t\u0361\u0283</code> <code>k</code> <code>(\u0294)</code> voiced <code>b</code> <code>d</code> <code>d\u0361\u0292</code> <code>\u0261</code> Fricative voiceless <code>(f)</code> <code>s</code> <code>(\u0283)</code> <code>(x)</code> <code>h</code> voiced <code>(v)</code> <code>(z)</code> Approximant <code>w</code> <code>l</code> <code>j</code> Trill <code>r</code> <p>On the other hand, it is quite tricky to say just exactly how many consonant phonemes there are. This is due to the heavy influence of both foreign languages like Dutch, Arabic, English and Sanskrit, as well as regional languages such as Balinese, Madurese, Sundanese, and Javanese <sup>1</sup>. They come in the form of loanwords whose phonemes are shown in parantheses.</p> <p>Regardless, if we account for all 24 consonant phonemes and the six vowel phonemes, we will arrive at a total of 30 phonemes. It is these 30 phonemes that serve as phonetic units in g2p ID, an Indonesian Grapheme-to-Phoneme Converter.</p>"},{"location":"blogs/accents_and_languages/#javanese","title":"Javanese","text":"<p>Javanese is one example of a difficult language to study. Not only does it vary from region to region (e.g. East versus West Javanese speakers), the language has also evolved from Old Javanese (Kawi) to the newer, modern Javanese. The table which we are going to analyze will supposedly cover phonemes of Modern Standard Javanese<sup>2</sup><sup>,</sup><sup>3</sup>.</p> <p>Vowel phonemes</p> Front Central Back Close <code>i</code> <code>u</code> Close-mid <code>e</code> <code>\u0259</code> <code>o</code> Open-mid <code>(\u025b)</code> <code>(\u0254)</code> Open <code>a</code> <p>Javanese vowel phonemes are essentially identical to that of Indonesian: six usual vowel phonemes plus two open-mid ones. There might additionally be phonetic changes, depending on where the speaker is located. For example, in the standard dialect of Surakarta, <code>a</code> is pronounced <code>\u0254</code> in word-final open syllables, and in any open penultimate syllable before such an <code>\u0254</code>.</p> <p>Consonant phoneme</p> Labial Dental/ Alveolar Retroflex Palatal Velar Glottal Nasal <code>m</code> <code>n</code> <code>\u0272</code> <code>\u014b</code> Plosive/ Affricate stiff voice <code>p</code> <code>t\u032a</code> <code>\u0288</code> <code>t\u0283</code> <code>k</code> <code>\u0294</code> slack voice <code>b\u0325</code> <code>d\u032a\u0325</code> <code>\u0256\u0325</code> <code>d\u0292\u030a</code> <code>\u0261\u030a</code> Fricative <code>s</code> <code>h</code> Semivowel <code>j</code> <code>w</code> Liquid lateral <code>l</code> rhotic <code>r</code> <p>Here's where it gets different: Javanese adds 2 new retroflex consonant phonemes <code>\u0288</code> and <code>\u0256\u0325</code> (romanized as <code>th</code> and <code>dh</code>, respectively), and eliminates most of the borrowed consonant phonemes that are present in Indonesian phonology. These differences influence the way several Javanese speakers speak Indonesian.</p> <p>For example, Javanese speakers tend to use the retroflex consonant phonemes in-place of the counterpart dental/alveolar ones. So instead of saying <code>medok</code>, a Javanese speaker may instead say <code>me\u0256\u0325ok</code>.</p> <p>On the flipside, the absence of <code>f</code> and <code>v</code> may cause the speaker to interchange with the phoneme <code>p</code>. For instance, instead of saying <code>b\u0259rpikir</code>, a Javanese speaker might say <code>b\u0259rfikir</code>. </p> <p>Certainly, these changes differ from region to region and from speaker to speaker, but it makes sense why Javanese speakers might have a strong accent when they speak Indonesian. Regional language speakers tend to incorporate their regional language's phonetic system into that of Indonesian's -- and Javanese speakers aren't the only ones who do that.</p>"},{"location":"blogs/accents_and_languages/#sundanese","title":"Sundanese","text":"<p>Like Javanese, the Sundanese language has evolved from the Old Sundanese script to a more modern version. Moreover, there are multiple dialects such as the Western Dialect (or Bantenese), Northern Dialect, Southern/Priangan Dialect, and still many others.</p> <p>Vowel phonemes</p> Front Central Back Close <code>i</code> <code>\u0268</code> <code>u</code> Mid <code>\u025b</code> <code>\u0259</code> <code>\u0254</code> Open <code>a</code> <p>Now, this is where Sundanese phonemes differ from that of Indonesian's and Javanese's. Aside from the usual 6 phonemes shared with Indonesian, Sundanese introduces a new vowel phoneme <code>\u0268</code> (romanized as <code>eu</code>). Examples of Sundanese words that contain the <code>\u0268</code> phoneme are: <code>teu</code>, <code>ieu</code>, <code>haseup</code>, <code>haseum</code>, etc.</p> <p>While it's not entirely obvious how this would bleed into the way a Sundanese speaker might speak Indonesian, some Indonesian words that do have their Sundanese counterparts (e.g. <code>asam</code> and <code>haseum</code>) might still be interchangeable and hence how the phoneme <code>\u0268</code> might be used.</p> <p>Consonant phonemes</p> Bilabial Alveolar Palatal Velar Glottal Nasal <code>m</code> <code>n</code> <code>\u0272</code> <code>\u014b</code> Plosive/ Affricate voiceless <code>p</code> <code>t</code> <code>t\u0283</code> <code>k</code> voiced <code>b</code> <code>d</code> <code>d\u0292</code> <code>\u0261</code> Fricative <code>s</code> <code>h</code> Lateral <code>l</code> Trill <code>r</code> Approximant <code>w</code> <code>j</code> <p>Like Javanese, Sundanese originally does not have borrow/foreign consonant phonemes and only have 18 consonants in total<sup>4</sup>. However, as foreign words were gradually incorporated into the language, several additional consonants such as <code>f</code>, <code>v</code>, <code>z</code>, <code>\u0283</code> and <code>x</code> have been introduced. And just like Javanese, these new phonemes tend to be transferred into native consonants, namely:</p> <ul> <li><code>f</code> / <code>v</code> <code>p</code></li> <li><code>\u0283</code> <code>s</code></li> <li><code>z</code> <code>d\u0292</code></li> <li><code>x</code> <code>h</code></li> </ul>"},{"location":"blogs/accents_and_languages/#toba-batak","title":"Toba Batak","text":"<p>Gradual language evolution and wide variation are no exceptions to Toba Batak, and basically to most regional languages of Indonesia as well. It used to also be written in Batak script but the Latin script is now preferred. </p> <p>Vowel phonemes<sup>5</sup></p> Front Central Back Close <code>i</code> <code>u</code> Close-mid <code>e</code> <code>(\u0259)</code> <code>o</code> Open-mid <code>\u025b</code> <code>\u0254</code> Open <code>a</code> <p>As shown above, Toba Batak has the same set of vowel phonemes to that of Indonesian and Javanese. However, the <code>\u0259</code> phoneme is not native to Toba Batak unlike the others! It only occurs in loanwords from Indonesian. Therefore, the tendency of native Toba Batak speakers is to pronounce all <code>e</code> graphemes as <code>e</code> phonemes. Hence, instead of saying <code>m\u0259reka baru k\u0259mbali</code>, they might say <code>mereka baru kembali</code> (notice the difference in <code>e</code>'s).</p> <p>This is particularly tricky as Indonesian is delicate on how the letter <code>e</code> is phonemized. A lot of Indonesian homographs depend on how the letter is spoken and could lead to different meanings. More on that here.</p> <p>Consonant phonemes</p> Labial Dental/ Alveolar (Alveolo-)palatal Velar Glottal Nasal <code>m</code> <code>n</code> <code>\u014b</code> Plosive/ Affricate voiceless <code>p</code> <code>t</code> <code>t\u0361\u0255</code> <code>k</code> voiced <code>b</code> <code>d</code> <code>d\u0361\u0291</code> <code>\u0261</code> Fricative <code>s</code> <code>h</code> Trill <code>r</code> Approximant <code>w</code> <code>l</code> <code>j</code> <p>Aside from the slight variation from palatal to alveolo-palatal consonant phonemes, Toba Batak has exactly the same consonant phonemes to that of Sundanese. Further, foreign borrowed consonant phonemes are also eliminated, hence phonetic transfers might also occur -- converting foreign phonemes (if any) to their closest native counterparts, just like Sundanese.</p>"},{"location":"blogs/accents_and_languages/#recap-takeaways-and-suggestions","title":"Recap, Takeaways, and Suggestions","text":"<p>A quick recap of the frequent changes that occur across the regional languages discussed above is as follows:</p> <ul> <li><code>t</code> <code>\u0288</code> (especially Javanese)</li> <li><code>d</code> <code>\u0256\u0325</code> (especially Javanese)</li> <li><code>a</code> <code>\u0254</code> (occassionally Javanese)</li> <li><code>f</code> / <code>v</code> <code>p</code></li> <li><code>\u0283</code> <code>s</code></li> <li><code>z</code> <code>d\u0292</code></li> <li><code>x</code> <code>h</code></li> <li><code>\u0259</code> <code>e</code> (especially Toba Batak)</li> </ul> <p>Now, these are only 3 out of hundreds of regional languages in Indonesia. But ultimately, the takeaway from exploring these different languages and their impact to the Indonesian accent is being contextually aware when building speech technology involving the language. Although it is widely understood that there is a \"standard Indonesian\" or \"accent-less Indonesian\", there might still be a vast majority of its speakers who tend to carry over their regional phonemization when speaking the lingua franca.</p> <p>The question to answer when building speech technology while knowing all of this is: should we build a system that accounts for these strong, varying accents? Do we mark them as non-Indonesian, or do we accept them for their differences and still consider them as proper Indonesian? Where is the fine line between a homographic mistake, versus a phonetic difference?</p> <p>We personally think that the answers are very much case-by-case dependent. If you could quickly identify the accent of the speaker, then having an accent-aware, stricter model would be perfect. But if that's not feasible, then having a more flexible, lenient model might be more preferrable.</p>"},{"location":"blogs/accents_and_languages/#extra-indonesian-english","title":"Extra: Indonesian English","text":"<p>Singaporeans have their own English accent, Singapore English (<code>en-SG</code>) and Malaysians similarly have Malaysian English (<code>en-MY</code>). Likewise, Indonesians have Indonesian English (<code>en-ID</code>): essentially how Indonesians tend to speak English, i.e. their accent.</p> <p>To the best of our knowledge, there is yet to be a concrete study investigating <code>en-ID</code>. We suspect that this is due to the fact that English is not Indonesia's official language, unlike its neighboring countries. However, MasteringBahasa noted several interesting characteristics of Indonesian English, which can be summarized as:</p> <ul> <li>Rolled R's</li> <li>No silent letters</li> <li>Transfers of English phonemes to native phonemes (<code>f</code>, <code>v</code>, <code>\u0283</code>)</li> <li>Full/whole A's</li> </ul> <p>You can find more details here.</p> <p>Written by Wilson Wongso. Last updated 14 December 2022.</p> <ol> <li> <p>Poedjosoedarmo, Soepomo (1982). \"Javanese influence on Indonesian phonology\". Javanese influence on Indonesian (PDF). D. Vol. 38. Canberra: Pacific Linguistics. pp. 19\u201350. Archived (PDF) from the original on 9 October 2022.\u00a0\u21a9</p> </li> <li> <p>Brown, Keith; Ogilvie, Sarah (2008). Concise encyclopedia of languages of the world. Elsevier. p. 560. ISBN 9780080877747.\u00a0\u21a9</p> </li> <li> <p>Suharno, Ignatius (1982). A Descriptive Study of Javanese. Canberra: ANU Asia-Pacific Linguistics / Pacific Linguistics Press. pp. 4\u20136. doi:10.15144/PL-D45. hdl:1885/145095. ISBN 9780858832589.\u00a0\u21a9</p> </li> <li> <p>M\u00fcller-Gotama, Franz (2001). Sundanese. Languages of the World. Materials. Vol. 369. Munich: LINCOM Europa.\u00a0\u21a9</p> </li> <li> <p>Nababan, P. W. J. (1981). A Grammar of Toba-Batak. Pacific Linguistics Series D \u2013 No. 37. Canberra: Dept. of Linguistics, Research School of Pacific Studies, The Australian National University. doi:10.15144/pl-d37. hdl:1885/145092.\u00a0\u21a9</p> </li> </ol>"},{"location":"blogs/bible_alignment/","title":"Bible Alignment","text":"<p>One data source often used in low resource languages is the Bible. Simply put, the Bible has been translated to thousands of languages, including languages which may be considered as extremely low resourced. Aside from the diverse set of languages, data is also abundant. Some translations of the Bible only contain New Testament, but that itself already contains ~7900 translation pairs. If it happens to include the Old Testament, then we can get up to ~31000 translation pairs in total.</p> <p>Languages of Indonesia is no exception, so we immediately saw the opportunity to leverage this.</p> <p>In this blog, I would like to document a few challenges that I found while aligning the Bible. To be specific, aligning here is the task of mapping one verse to another. For instance, we map the Indonesian translation of Genesis 1:1 to that of the Javanese translation. On paper, this sounds easy to do. Indeed, the majority of Bible verses align very well. By that I mean Genesis 1:1 in Indonesian really does correspond to the Javanese version.</p> <p>But upon closer inspection, there are variations in how the verses are presented.</p>"},{"location":"blogs/bible_alignment/#variation-1-cascading-verses","title":"Variation 1: Cascading Verses","text":"<p>I would call this first type of variation as \"cascading\" verses. Note that there may be an official term for this, but calling them \"cascading\" verses is just the most intuitive one for me.</p> <p>Take Exodus 6 in Javanese, for example.</p> # Verse Exodus 6:1 (5-24) Nanging pangandikane Pangeran Yehuwah marang Nabi Musa: \u201cIng samengko sira bakal sumurup, apa kang bakal Suntandukake marang Pringon; anggone bakal nglilani lunga bangsa iki sarana dipeksa ing asta kang rosa, iya marga saka dipeksa dening asta kang rosa dheweke bakal nundhung bangsa iku saka ing nagarane.\u201d Exodus 6:2 (6-1) Sabanjure Gusti Allah ngandika marang Nabi Musa mangkene: \u201cIngsun iki Yehuwah. Exodus 6:3 (6-2) Ingsun wus ngatingal marang Abraham, Iskak lan Yakub, dadi Allah kang Mahakuwasa, nanging asmaningSun Yehuwah durung Sunsumurupake. Exodus 6:4 (6-3) Karodene Ingsun ora mung damel prasetyan karo leluhurira iku, yen bakal Sunparingi tanah Kanaan, tanah panggonane neneka, <p>Notice that in front of each verse, there is an additional verse identifier. It says that the verse in Exodus 6:1 is, actually Exodus 5:24? Because of that \"shift\", every other verse after that follows and hence \"cascades\".</p> <p>Note</p> <p>Interestingly, the additional verse identifier is removed in the NIV version.</p> <p>Note that if this shift is inconsistent across languages, it would certainly cause issues: verses are therefore not parallel and hence not valid translation pairs. Otherwise, we can simply drop these additional verse identifiers.</p> <p>What I noticed that for this case (Exodus 1:6) in particular, the \"shift\", if identified, is parallel across languages:</p> Language Version Verse Javanese Jawa (5-24) Nanging pangandikane Pangeran Yehuwah marang Nabi Musa: \u201cIng samengko sira bakal sumurup, apa kang bakal Suntandukake marang Pringon; anggone bakal nglilani lunga bangsa iki sarana dipeksa ing asta kang rosa, iya marga saka dipeksa dening asta kang rosa dheweke bakal nundhung bangsa iku saka ing nagarane.\u201d Indonesian Terjemahan Baru (TB) (5-24) Tetapi TUHAN berfirman kepada Musa: \"Sekarang engkau akan melihat, apa yang akan Kulakukan kepada Firaun; sebab dipaksa oleh tangan yang kuat ia akan membiarkan mereka pergi, ya dipaksa oleh tangan yang kuat ia akan mengusir mereka dari negerinya.\" Sundanese Sunda PANGERAN ngandika ka Musa, \"Tenjokeun ku maneh rek dikumahakeun eta raja teh ku Kami. Saenya-enyana, ku Kami rek dipaksa nepi ka manehna kapaksa ngaluarkeun umat Kami ti nagarana.\" Madurese Madura (5-24) Dhabuna PANGERAN ka Mosa, \"Sateya ba\u2019na bakal nangaleyana epabaramma\u2019a rato jareya bi\u2019 Sengko\u2019. Rato jareya bi\u2019 Sengko\u2019 epaksa\u2019a mabebas Tang bangsa. Saongguna, bi\u2019 Sengko\u2019 rato jareya epaksa\u2019a ngojuk Tang bangsa jareya dhari nagara reya.\" Balinese Bali Ida Sang Hyang Widi Wasa raris ngandika ring Dane Musa: \u201cAne jani kita lakar nepukin saluiring ane lakar laksanayang Ulun marep teken sang prabu. Ulun lakar maksa ia, apang ia maang kaulan Ulune makaad. Sasajaane Ulun lakar maksa ia apanga ia nundung kaulan Ulune uli guminnyane.\u201d"},{"location":"blogs/bible_alignment/#variation-2-combined-verses","title":"Variation 2: Combined Verses","text":"<p>I would call the second type of variation as \"combined verses\". At times we can sort of consider this as a derivation of variation #1. An example of this is Revelations 13:1, again, in Javanese.</p> # Verse Revelations 13:1 (12-18) lan banjur manggon ana ing pinggir sagara. (13-1) Sabanjure aku weruh ana kewan njedhul saka sajroning sagara, sungune sapuluh lan endhase pitu; pucuking sungune ana makuthane sapuluh, lan ing endhase ana cirine jeneng panyenyamah. Revelations 13:2 Kewan kang dakdeleng iku rupane kaya macan tutul, sikile kaya sikil bruwang lan cangkeme kaya cangkem singa. Iku banjur diwenehi kasekten dening naga mau, lan dhampar tuwin panguwasane kang gedhe. Revelations 13:3 Aku nuli ndeleng endhase kang siji iku kaya ana tatune kang mbebayani tumrap uripe, nanging tatune banjur waras. Temah wong sadonya padha kaeraman, banjur ngetut buri kewan iku. <p>Notice that Revelations 13:1 contains both Revelations 12:18 and 13:1. We might presume that verses will cascade, but in this case, they don't. Now, the questions are:</p> <ol> <li>Do we assign the sub-verse 12:18 as \"Revelations 12:18\" explicitly? Or do we consider it part of Revelations 13:1?</li> <li>How do other languages handle this?</li> </ol> <p>I would opt to answer the second question first, since that'll be important to ultimately decide whether it is parallel, i.e., if is a valid translation pair. With a simple verse comparison, we find:</p> Language Version Verse Javanese Jawa (12-18) lan banjur manggon ana ing pinggir sagara. (13-1) Sabanjure aku weruh ana kewan njedhul saka sajroning sagara, sungune sapuluh lan endhase pitu; pucuking sungune ana makuthane sapuluh, lan ing endhase ana cirine jeneng panyenyamah. Indonesian Terjemahan Baru (TB) (12-18) Dan ia tinggal berdiri di pantai laut. (13-1) Lalu aku melihat seekor binatang keluar dari dalam laut, bertanduk sepuluh dan berkepala tujuh; di atas tanduk-tanduknya terdapat sepuluh mahkota dan pada kepalanya tertulis nama-nama hujat. Sundanese Sunda (12-18) Gen naga ngajanteng di sisi basisir. (13-1) Ti dinya kaula nenjo aya hiji sato hanjat ti laut, huluna tujuh tandukna sapuluh. Unggal tanduk make makuta, dina unggal huluna aya tulisan hiji ngaran anu ngahina ka Allah. Madurese Madura (12-18) Naga ganeka laju manjeng e paseser. (13-1) Kaula pas nengale badha keban raja kalowar dhari dhalem tase\u2019. Keban ganeka atandhu\u2019 sapolo ban acethak papetto\u2019. E tandhu\u2019na se sapolo ganeka badha jamang settong ebang, ban e saneyap cethagga badha tolesanna, aropa nyama panyeya\u2019an ka Allah. Balinese Bali (12-18) Nagane punika raris nyeleg ring sisin segarane. (13-1) Tumuli tiang ngatonang sato asiki medal saking tengah segarane. Satone punika matanduk adasa, tur tenggekipune pepitu. Sabilang tanduknyane madaging gegelungan asiki. Tur ring sabilang tenggeknyane matulis satunggiling wasta kanistan. <p>Luckily, this verse combination is consistent across versions. Thus, whether or not we decide to split the verse into two subverses may not really matter here because parallelism is maintained.</p> <p>Note</p> <p>Similarly, those additional verse identifiers are removed in the NIV version.</p> <p>But, if we're opting for a simple solution, again, we can just drop the additional verse identifier. Therefore, we retain the same \"drop-verse-identifier\" method we opted for variation #1.</p>"},{"location":"blogs/bible_alignment/#variation-3-verse-ranges","title":"Variation 3: Verse Ranges","text":"<p>Now, the last variation is the one that I find to be the most problematic due to its inconsistency across different versions of the Bible. If I'm not mistaken, this variation is known as \"verse ranges\", where one text may correspond to more than one verse identifiers. I will take Acts 8:28 as an example:</p> Language Version Verse Javanese Jawa Nalika samono panjenengane lagi tindak kondur nitih kreta karo maos kitabe Nabi Yesaya. Indonesian Terjemahan Baru (TB) Sekarang orang itu sedang dalam perjalanan pulang dan duduk dalam keretanya sambil membaca kitab nabi Yesaya. Sundanese Sunda (8:27) Madurese Madura (8:27) Balinese Bali (8:27) <p>Notice that in languages like Sundanese, Madurese, and Balinese, Acts 8:28 is just pointing to Acts 8:27. This is because, in those versions, the \"text\" in Acts 8:27 actually corresponds to Acts 8:27-28, both verses. But observe that this is not the case for other languages! Clearly, the Javanese and Indonesian versions do not have that \"verse range\". Instead, there is an explicit split/distinction between verses 27 and 28.</p> <p>See the following table<sup>1</sup> for a comparison of how verse ranges differ across languages:</p> Version # Verse New International Version (NIV) Acts 8:27 So he started out, and on his way he met an Ethiopian eunuch, an important official in charge of all the treasury of Candace, queen of the Ethiopians. This man had gone to Jerusalem to worship, Acts 8:28 and on his way home was sitting in his chariot reading the book of Isaiah the prophet. Terjemahan Baru (TB) Acts 8:27 Lalu berangkatlah Filipus. Adalah seorang Etiopia, seorang sida-sida, pembesar dan kepala perbendaharaan Sri Kandake, ratu negeri Etiopia, yang pergi ke Yerusalem untuk beribadah.  Acts 8:28 Sekarang orang itu sedang dalam perjalanan pulang dan duduk dalam keretanya sambil membaca kitab nabi Yesaya. Sunda Acts 8:27 (8:27-28) Pilipus geuwat angkat. Di eta jalan aya hiji pajabat luhur urang Etiopia, anu ngurus harta kakayaan Sri Kandasi ratu nagri Etiopia, tas ti Yerusalem ngalakonan ibadah. Eta pajabat tunggang kareta seja mulih ka nagarana, sajajalan ngaos Kitab Nabi Yesaya. Acts 8:28 (8:27) Bali Acts 8:27 Irika Dane Pilipus makinkin raris mamargi. Duk punika wenten prakangge agung saking jagat Etiopia nuju mamargi mantuk. Prakangge agung punika, dados prakangge buat ngetangang druen Sri Kandake, Sang Raja Putri ring jagat Etiopia. Dane sampun lunga ka kota Yerusalem ngaturang bakti ring Ida Sang Hyang Widi Wasa, tur sane mangkin dane mawali mantuk nglinggihin kreta. Sajeroning pamargin danene punika, dane ngwacen cakepan dane Nabi Yesaya. Acts 8:28 (8:27) <p>You can quickly observe that verse ranges are highly inconsistent. In versions like NIV and TB, the verses are separated. But for those that are combined, the point where they split is not stated, i.e., verses are grouped into \"ranges\".</p> <p>How to best approach this variation remains unclear for the time being. If we were to ignore the verse identifiers just like in variations #1 and #2 above, we would end up producing verses that don't align to each other. One verse may contain \"additional texts\" that seem to just be \"neglected\" during the translation process -- where in fact, that's certainly not the case.</p> <p>One solution that I could think of is mark verses with ranges as actual verse ranges, instead of treating them as a single verse. The updated version of the table above should describe this better:</p> Version # Verse Sunda Acts 8:27 Acts 8:27-28 Pilipus geuwat angkat. Di eta jalan aya hiji pajabat luhur urang Etiopia, anu ngurus harta kakayaan Sri Kandasi ratu nagri Etiopia, tas ti Yerusalem ngalakonan ibadah. Eta pajabat tunggang kareta seja mulih ka nagarana, sajajalan ngaos Kitab Nabi Yesaya. Acts 8:28 (8:27)  Bali Acts 8:27 Acts 8:27-28 Irika Dane Pilipus makinkin raris mamargi. Duk punika wenten prakangge agung saking jagat Etiopia nuju mamargi mantuk. Prakangge agung punika, dados prakangge buat ngetangang druen Sri Kandake, Sang Raja Putri ring jagat Etiopia. Dane sampun lunga ka kota Yerusalem ngaturang bakti ring Ida Sang Hyang Widi Wasa, tur sane mangkin dane mawali mantuk nglinggihin kreta. Sajeroning pamargin danene punika, dane ngwacen cakepan dane Nabi Yesaya. Acts 8:28 (8:27)  <p>Notice that we can not only drop verse identifiers, which was the solution we chose for the two aforementioned variations, but also avoid aligning verses that do not correspond. We can treat the new verse ranges as the \"primary key\" of that text.</p> <p>Therefore, when we do a table join with other Bible versions that do not have verse ranges, the invalid translation pair will not be produced. On the flipside, when we match Bible versions that have the same verse ranges as primary keys, the two will match and hence produce a new valid translation pair.</p> <p>We can apply the same solution for ranges that span more than two verses, it will just be a longer text. This also remains applicable even if there may be sub-splits of verses (e.g. verse 6a, 6b, etc.). Taking Matthew 1 in Sundanese as an example, we can modify:</p> # Verse Matthew 1:1 Ieu daptar karuhun-karuhun Yesus Kristus turunan Daud, ari Daud turunan Ibrahim. Matthew 1:2 (1:2-6a) Ti Ibrahim turun-tumurun nepi ka Raja Daud kieu pareleanana: Ibrahim, Ishak, Yakub, Yuda jeung saderek-saderekna, Peres, Serah (ti ibu Tamar), Hesron, Ram, Aminadab, Nahson, Salmon, Boas (ibuna Boas nya eta Rahab), Obed, Isai, Raja Daud. Matthew 1:3 (1:2) Matthew 1:4 (1:2) Matthew 1:5 (1:2) Matthew 1:6 (1:6b-11) Ti Daud turun-tumurun nepi ka jaman urang Israil diboyong ka Babul kieu pareleanana: Daud ti geureuhana anu asalna garwa Uria, puputra Suleman, ti Suleman turun-tumurun: Rehabam, Abia, Asa, Yosapat, Yehoram, Usia, Yotam, Ahas, Hiskia, Menase, Amon, Yosia, Yoyakin jeung saderek-saderekna. Matthew 1:7 (1:6) Matthew 1:8 (1:6) Matthew 1:9 (1:6) Matthew 1:10 (1:6) Matthew 1:11 (1:6) <p>as:</p> # Verse Matthew 1:1 Ieu daptar karuhun-karuhun Yesus Kristus turunan Daud, ari Daud turunan Ibrahim. Matthew 1:2-5 Ti Ibrahim turun-tumurun nepi ka Raja Daud kieu pareleanana: Ibrahim, Ishak, Yakub, Yuda jeung saderek-saderekna, Peres, Serah (ti ibu Tamar), Hesron, Ram, Aminadab, Nahson, Salmon, Boas (ibuna Boas nya eta Rahab), Obed, Isai, Raja Daud. Matthew 1:6-11 Ti Daud turun-tumurun nepi ka jaman urang Israil diboyong ka Babul kieu pareleanana: Daud ti geureuhana anu asalna garwa Uria, puputra Suleman, ti Suleman turun-tumurun: Rehabam, Abia, Asa, Yosapat, Yehoram, Usia, Yotam, Ahas, Hiskia, Menase, Amon, Yosia, Yoyakin jeung saderek-saderekna. <p>Interestingly, the Balinese version of Matthew 1:1-11 are not ranged (!), unlike the Sundanese version, despite the fact that they do have similar ranges in Acts 8:27-28 as we have just seen earlier. We could potentially avoid non-matching translation pairs by assigning a verse range as primary key.</p>"},{"location":"blogs/bible_alignment/#closing-thoughts","title":"Closing Thoughts","text":"<p>If my hypothesis is correct and there remains no other variations to be kept in mind, then we should be able to produce a decently aligned<sup>2</sup> Bible translation dataset, which we then can use to train a neural machine translation (NMT) model.</p> <p>Written by Wilson Wongso. Last updated 12 December 2022.</p> <ol> <li> <p>Highlights added manually. Highlights are also just estimates, they may not correspond/align perfectly.\u00a0\u21a9</p> </li> <li> <p>\"Decently aligned\" because Bible verses may not even be 1-to-1 to begin with! How much this impacts the quality of the translation model could be further investigated.\u00a0\u21a9</p> </li> </ol>"},{"location":"blogs/launch/","title":"Launching LazarusNLP: Reviving Indonesia's Dying Languages through NLP","text":"<p>Today we are launching LazarusNLP, an independent research group dedicated to leveraging Natural Language Processing (NLP) to preserve and revitalize the diverse languages of Indonesia. In a nation boasting over 700 distinct languages, our mission is to ensure that each language receives the attention it deserves in the digital age.</p> <p>This blog aims to discuss the gaps in NLP research and development for Indonesian languages and introduce our initial projects. We are excited to share our work and invite the community to join us in our mission!</p> <p>You can try out our projects in the following web app demo:</p> <p> </p> <p>Info</p> <p>This web app is available at our \ud83e\udd17 HuggingFace Space.</p>"},{"location":"blogs/launch/#background","title":"Background","text":"<p>Indonesia's linguistic landscape is rich and varied, with languages evolving independently across different regions. Despite the prevalence of Indonesian (Bahasa Indonesia) as the national language, many of these regional languages face the threat of extinction. UNESCO has identified 137 Indonesian languages as vulnerable or endangered, highlighting the urgent need for action<sup>1</sup>.</p> <p>While advancements in NLP have benefited major languages like Indonesian, there has been a glaring gap in applying these technologies to Indonesia's regional languages. This neglect risks further marginalizing these languages in an increasingly digital world. Our mission is to bridge this gap by developing NLP tools and resources tailored to the unique linguistic features of each Indonesian language and to reviving Indonesia's dying languages.</p>"},{"location":"blogs/launch/#projects","title":"Projects","text":""},{"location":"blogs/launch/#indot5-t5-language-models-for-the-indonesian-language","title":"IndoT5: T5 Language Models for the Indonesian Language","text":"<p>IndoT5 is a T5-based language model trained specifically for the Indonesian language. With just 8 hours of training on a limited budget, we developed a competitive sequence-to-sequence, encoder-decode model capable of fine-tuning tasks such as summarization, chit-chat, and question-answering. Despite the limited training constraints, our model is competitive when evaluated on the IndoNLG (text generation) benchmark.</p> <ul> <li> GitHub Repository</li> <li>\ud83e\udd17 HuggingFace Collection</li> </ul>"},{"location":"blogs/launch/#indonesian-sentence-embedding-models","title":"Indonesian Sentence Embedding Models","text":"<p>We trained open-source sentence embedding models for Indonesian, enabling applications such as information retrieval (useful for retrieval-augmented generation!) semantic text similarity, and zero-shot text classification. We leverage existing pre-trained Indonesian language models like IndoBERT and state-of-the-art unsupervised techniques and established sentence embedding benchmarks.</p> <ul> <li> GitHub Repository</li> <li> Documentation</li> <li>\ud83e\udd17 HuggingFace Collection</li> </ul>"},{"location":"blogs/launch/#indonesian-natural-language-inference-nli-models","title":"Indonesian Natural Language Inference (NLI) Models","text":"<p>Open-source lightweight NLI models that are competitive with larger models on IndoNLI benchmark, with significantly less parameters. We applied knowledge distillation methods to small existing pre-trained language models like IndoBERT Lite. These models offer efficient solutions for tasks requiring natural language inference capabilities while minimizing computational resources such as cross-encoder-based semantic search.</p> <ul> <li>\ud83e\udd17 HuggingFace Collection</li> </ul>"},{"location":"blogs/launch/#many-to-many-multilingual-translation-models","title":"Many-to-Many Multilingual Translation Models","text":"<p>Adapting mT5 to 45 languages of Indonesia, we developed a robust baseline model for multilingual translation for languages of Indonesia. This facilitates further fine-tuning for niche domains and low-resource languages, contributing to greater linguistic inclusivity. Our models are competitive with existing multilingual translation models on the NusaX benchmark.</p> <ul> <li> GitHub Repository</li> <li>\ud83e\udd17 HuggingFace Collection</li> </ul>"},{"location":"blogs/launch/#future-plans","title":"Future Plans","text":"<p>Our journey has just begun. Looking ahead, we are committed to expanding our repository of open-source pre-trained language models, with a focus on Indonesia's languages, multilinguality, culture, and code-switching. By democratizing access to NLP tools for all Indonesian languages, we aim to catalyze a renaissance in linguistic diversity.</p> <p>Join us in our mission to breathe new life into Indonesia's linguistic tapestry!</p>"},{"location":"blogs/launch/#contact-us","title":"Contact Us","text":"<p>We are always open to collaboration and welcome contributions from the community. If you are interested in our work or have ideas to share, please reach out to us at lazarusnlp(at)gmail(dot)com.</p> <p>Written by David Samuel Setiawan, Steven Limcorn, and Wilson Wongso. Last updated 19 February 2024.</p> <ol> <li> <p>Moseley, Christopher, ed. (2010). Atlas of the World\u2019s Languages in Danger. Memory of Peoples (3rd ed.). Paris: UNESCO Publishing. ISBN 978-92-3-104096-2.\u00a0\u21a9</p> </li> </ol>"},{"location":"projects/machine-translation/","title":"Machine Translation","text":"<ul> <li> GitHub Repository</li> <li>\ud83e\udd17 HuggingFace Collection</li> </ul>"},{"location":"projects/machine-translation/#indo-mt5","title":"Indo-mT5","text":"<p>Indo-mT5 is mT5 fine-tuned for machine translation of regional languages of Indonesia. We release our dataset creation scripts, training code, and fine-tuned models for other to leverage.</p> <p>There are two types of models:</p> <ul> <li>Multilingual: Many-to-many, multilingual translation model.</li> <li>Bilingual: Unidirectional, bilingual translation model.</li> </ul> <p>We also further experiment with two settings:</p> <ul> <li>Baseline: Model trained on 7 languages (<code>ace</code>, <code>ban</code>, <code>bug</code>, <code>ind</code>, <code>jav</code>, <code>min</code>, <code>sun</code>).</li> <li>All: Model trained on 45 languages as listed here.</li> </ul>"},{"location":"projects/machine-translation/#training","title":"Training","text":"<p>Our experiments are conducted in these steps:</p> <ul> <li>Multilingual Training on Bible: We first fine-tuned mT5 on multilingual translation on parallel Bible dataset, creating Indo-mT5.</li> <li>Multilingual Training on NusaX: We take Indo-mT5 and fine-tune them on multilingual pairs of the NusaX dataset.</li> <li>Bilingual Training on NusaX: We take Indo-mT5 and fine-tune them on bilingual pairs of the NusaX dataset.</li> </ul> <p>Therefore, we have six training scripts:</p> Dataset Config Type Training Script Evaluation Script Bible Baseline Multilingual train_bible_baseline.sh eval_bible_baseline.sh Bible All (v2) Multilingual train_bible_all.sh eval_bible_all.sh NusaX Baseline Multilingual train_nusax_baseline_multilingual.sh eval_nusax_baseline_multilingual.sh NusaX All (v2) Multilingual train_nusax_all_multilingual.sh eval_nusax_all_multilingual.sh NusaX Baseline Bilingual train_nusax_baseline_bilingual.sh eval_nusax_baseline_bilingual.sh NusaX All (v2) Bilingual train_nusax_all_bilingual.sh eval_nusax_all_bilingual.sh"},{"location":"projects/machine-translation/#results","title":"Results","text":"<p>We evaluated our models on NusaX (Winata et al., 2022) and compared them to existing models.</p>"},{"location":"projects/machine-translation/#ind-x","title":"<code>ind -&gt; x</code>","text":"Model #params <code>ace</code> <code>ban</code> <code>bbc</code> <code>bjn</code> <code>bug</code> <code>jav</code> <code>mad</code> <code>min</code> <code>nij</code> <code>sun</code> avg IndoGPT (Winata et al., 2022) 117M 9.60 14.17 8.20 22.23 5.18 24.05 14.44 26.95 17.56 23.15 16.55 IndoBART v2 (Winata et al., 2022) 132M 19.21 27.08 18.41 40.03 11.06 39.97 28.95 48.48 27.11 38.46 29.88 mBART-50 Large (Winata et al., 2022) 610M 17.21 22.67 17.79 34.26 10.78 35.33 28.63 43.87 25.91 31.21 26.77 mT5 Base (Winata et al., 2022) 580M 14.79 18.07 18.22 38.64 6.68 33.48 0.96 45.84 13.59 33.79 22.41 NLLB-200 Distilled (zero-shot) 600M 2.74 4.87 - - 1.66 17.66 - 9.79 - 11.92 8.11 Indo-mT5 NusaX Multilingual 580M 16.02 22.48 - - 8.86 33.65 - 33.65 - 29.76 24.07 Indo-mT5 NusaX Bilingual 580M 17.99 27.03 - - 10.80 39.63 - 51.56 - 35.16 30.36 Indo-mT5 v2 NusaX Multilingual 580M 14.28 19.19 14.86 28.39 8.05 28.70 20.95 32.70 22.30 26.19 21.56 Indo-mT5 v2 NusaX Bilingual 580M 17.58 24.24 16.69 38.81 10.20 37.87 29.77 50.90 26.93 34.22 28.72"},{"location":"projects/machine-translation/#x-ind","title":"<code>x -&gt; ind</code>","text":"Model #params <code>ace</code> <code>ban</code> <code>bbc</code> <code>bjn</code> <code>bug</code> <code>jav</code> <code>mad</code> <code>min</code> <code>nij</code> <code>sun</code> avg IndoGPT (Winata et al., 2022) 117M 7.01 13.23 5.27 19.53 1.98 27.31 13.75 23.03 10.83 23.18 14.51 IndoBART v2 (Winata et al., 2022) 132M 24.44 40.49 19.94 47.81 12.64 50.64 36.10 58.38 33.50 45.96 36.99 mBART-50 Large (Winata et al., 2022) 610M 18.45 34.23 17.43 41.73 10.87 39.66 32.11 59.66 29.84 35.19 31.92 mT5 Base (Winata et al., 2022) 580M 18.59 21.73 12.85 42.29 2.64 45.22 32.35 58.65 25.61 36.58 29.65 NLLB-200 Distilled (zero-shot) 600M 9.42 21.24 - - 6.18 30.54 - 40.49 - 26.91 22.46 Indo-mT5 NusaX Multilingual 580M 23.94 35.30 - - 16.68 29.76 - 48.10 - 36.54 31.72 Indo-mT5 NusaX Bilingual 580M 24.78 42.15 - - 16.27 47.26 - 62.94 - 42.39 39.30 Indo-mT5 v2 NusaX Multilingual 580M 21.01 30.43 18.57 34.21 14.42 35.19 27.04 42.64 26.90 33.78 28.42 Indo-mT5 v2 NusaX Bilingual 580M 22.87 39.48 20.48 44.53 15.97 45.20 36.65 60.97 32.38 39.80 35.83"},{"location":"projects/machine-translation/#parallel-bible-dataset-creation","title":"Parallel Bible Dataset Creation","text":"<p>This will cover the creation process of our Bible machine-translation dataset.</p>"},{"location":"projects/machine-translation/#overview","title":"Overview","text":"<ol> <li>Scrape Bible Data</li> <li>Align Bible Verses</li> <li>Load as Machine-Translation Dataset</li> </ol>"},{"location":"projects/machine-translation/#bible-scraping","title":"Bible Scraping","text":"<pre><code>python utils/scrape_parallel.py \\\n    --codes abun aceh ambdr aralle balantak bali bambam bauzi berik bugis dairi duri ende galela gorontalo iban jawa kaili_daa karo kupang lampung madura makasar mamasa manggarai mentawai meyah minang mongondow napu ngaju nias rote sabu sangir sasak simalungun sunda taa tabaru tb toba toraja uma yali yawa \\\n    --outdir corpus \\\n    -j 4\n</code></pre>"},{"location":"projects/machine-translation/#align-bible-verses","title":"Align Bible Verses","text":"<pre><code>for LANGUAGE in abun aceh ambdr aralle balantak bali bambam bauzi berik bugis dairi duri ende galela gorontalo iban jawa kaili_daa karo kupang lampung madura makasar mamasa manggarai mentawai meyah minang mongondow napu ngaju nias rote sabu sangir sasak simalungun sunda taa tabaru tb toba toraja uma yali yawa\ndo\n    python utils/align.py --path corpus/$LANGUAGE.json --outdir corpus_aligned\ndone\n</code></pre> <p>You can read more about aligning Bible verses in our blogpost.</p>"},{"location":"projects/machine-translation/#data-loading-script","title":"Data Loading Script","text":"<p>In the data loading script, we have to do these two steps:</p> <ol> <li>Split unique verse IDs into train/test/validation subsets.</li> <li>Generate permutations of every verse ID for every subset.</li> </ol> <p>You can find our data loading implementation in src/alkitab-sabda-mt.py.</p>"},{"location":"projects/nusabert/","title":"NusaBERT","text":"<ul> <li> GitHub Repository</li> <li>\ud83e\udd17 HuggingFace Collection</li> </ul> <p>This project aims to extend the multilingual and multicultural capability of IndoBERT (Wilie et al., 2020). We expanded the IndoBERT tokenizer on 12 new regional languages of Indonesia, and continued pre-training on a large-scale corpus consisting of the Indonesian language and 12 regional languages of Indonesia. Our models are highly competitive and robust on multilingual and multicultural benchmarks, such as IndoNLU, NusaX, and NusaWrites.</p> <p> </p>"},{"location":"projects/nusabert/#pre-trained-models","title":"Pre-trained Models","text":"Model #params Dataset LazarusNLP/NusaBERT-base 111M sabilmakbar/indo_wiki, acul3/KoPI-NLLB, uonlp/CulturaX LazarusNLP/NusaBERT-large 337M sabilmakbar/indo_wiki, acul3/KoPI-NLLB, uonlp/CulturaX"},{"location":"projects/nusabert/#results","title":"Results","text":"<p>We evaluate our models on three benchmarks: IndoNLU, NusaX, and NusaWrites, which measures the model's natural language understanding, multilingual, and multicultural capabilities. The datasets supports a variety of languages of Indonesia. </p> <p>The values on the table below denotes the F1 score on the test set.</p>"},{"location":"projects/nusabert/#indonlu-classification","title":"IndoNLU (Classification)","text":"Model EmoT SmSA CASA HoASA WReTE AVG mBERT 67.30 84.14 72.23 84.63 84.40 78.54 XLM-MLM 65.75 86.33 82.17 88.89 64.35 77.50 XLM-R Base 71.15 91.39 91.71 91.57 79.95 85.15 XLM-R Large 78.51 92.35 92.40 94.27 83.82 88.27 IndoBERT Lite Base p1 73.88 90.85 89.68 88.07 82.17 84.93 IndoBERT Lite Base p2 72.27 90.29 87.63 87.62 83.62 84.29 IndoBERT Base p1 75.48 87.73 93.23 92.07 78.55 85.41 IndoBERT Base p2 76.28 87.66 93.24 92.70 78.68 85.71 IndoBERT Lite Large p1 75.19 88.66 90.99 89.53 78.98 84.67 IndoBERT Lite Large p2 70.80 88.61 88.13 91.05 85.41 84.80 IndoBERT Large p1 77.08 92.72 95.69 93.75 82.91 88.43 IndoBERT Large p2 79.47 92.03 94.94 93.38 80.30 88.02 Our work LazarusNLP/NusaBERT-base 76.10 87.46 91.26 89.80 76.77 84.28 LazarusNLP/NusaBERT-large 78.90 87.36 92.13 93.18 82.64 86.84"},{"location":"projects/nusabert/#indonlu-sequence-labeling","title":"IndoNLU (Sequence Labeling)","text":"Model POSP BaPOS TermA KEPS NERGrit NERP FacQA AVG mBERT 91.85 83.25 89.51 64.31 75.02 69.27 61.29 76.36 XLM-MLM 95.87 88.40 90.55 65.35 74.75 75.06 62.15 78.88 XLM-R Base 95.16 84.64 90.99 68.82 79.09 75.03 64.58 79.76 XLM-R Large 92.73 87.03 91.45 70.88 78.26 78.52 74.61 81.92 IndoBERT Lite Base p1 91.40 75.10 89.29 69.02 66.62 46.58 54.99 70.43 IndoBERT Lite Base p2 90.05 77.59 89.19 69.13 66.71 50.52 49.18 70.34 IndoBERT Base p1 95.26 87.09 90.73 70.36 69.87 75.52 53.45 77.47 IndoBERT Base p2 95.23 85.72 91.13 69.17 67.42 75.68 57.06 77.34 IndoBERT Lite Large p1 91.56 83.74 90.23 67.89 71.19 74.37 65.50 77.78 IndoBERT Lite Large p2 94.53 84.91 90.72 68.55 73.07 74.89 62.87 78.51 IndoBERT Large p1 95.71 90.35 91.87 71.18 77.60 79.25 62.48 81.21 IndoBERT Large p2 95.34 87.36 92.14 71.27 76.63 77.99 68.09 81.26 Our work LazarusNLP/NusaBERT-base 95.77 96.02 90.54 66.67 72.93 82.29 54.81 79.86 LazarusNLP/NusaBERT-large 96.89 96.76 91.73 71.53 79.86 85.12 66.77 84.09"},{"location":"projects/nusabert/#nusax","title":"NusaX","text":"Model <code>ace</code> <code>ban</code> <code>bbc</code> <code>bjn</code> <code>bug</code> <code>eng</code> <code>ind</code> <code>jav</code> <code>mad</code> <code>min</code> <code>nij</code> <code>sun</code> AVG Naive Bayes 72.5 72.6 73.0 71.9 73.7 76.5 73.1 69.4 66.8 73.2 68.8 71.9 72.0 SVM 75.7 75.3 76.7 74.8 77.2 75.0 78.7 71.3 73.8 76.7 75.1 74.3 75.4 Logistic Regression 77.4 76.3 76.3 75.0 77.2 75.9 74.7 73.7 74.7 74.8 73.4 75.8 75.4 IndoNLU IndoBERT Base 75.4 74.8 70.0 83.1 73.9 79.5 90.0 81.7 77.8 82.5 75.8 77.5 78.5 IndoNLU IndoBERT Large 76.3 79.5 74.0 83.2 70.9 87.3 90.2 85.6 77.2 82.9 75.8 77.2 80.0 IndoLEM IndoBERT Base 72.6 65.4 61.7 71.2 66.9 71.2 87.6 74.5 71.8 68.9 69.3 71.7 71.1 mBERT Base 72.2 70.6 69.3 70.4 68.0 84.1 78.0 73.2 67.4 74.9 70.2 74.5 72.7 XLM-R Base 73.9 72.8 62.3 76.6 66.6 90.8 88.4 78.9 69.7 79.1 75.0 80.1 76.2 XLM-R Large 75.9 77.1 65.5 86.3 70.0 92.6 91.6 84.2 74.9 83.1 73.3 86.0 80.0 Our work LazarusNLP/NusaBERT-base 76.51 78.67 74.02 82.38 71.64 84.09 89.74 84.09 75.62 80.77 74.93 85.21 79.81 LazarusNLP/NusaBERT-large 81.8 82.83 74.71 86.51 73.36 84.63 93.33 87.20 82.50 83.54 77.72 82.74 82.57"},{"location":"projects/nusabert/#nusawrites-nusaparagraph","title":"NusaWrites (NusaParagraph)","text":"Models Emotion Rhetorical Mode Topic Naive Bayes 75.51 37.73 85.06 SVM 76.36 45.44 85.86 Logistic Regression 78.23 45.21 87.67 IndoNLU IndoBERT Base 67.12 47.92 85.87 IndoNLU IndoBERT Large 62.65 31.75 85.41 IndoLEM IndoBERT Base 66.94 51.93 84.87 mBERT 63.15 50.01 73.82 XLM-R Base 59.15 49.17 71.68 XLM-R Large 67.42 51.57 83.05 Our work LazarusNLP/NusaBERT-base 67.18 51.34 84.17 LazarusNLP/NusaBERT-large 71.82 53.06 85.89"},{"location":"projects/nusabert/#nusawrites-nusatranslation","title":"NusaWrites (NusaTranslation)","text":"Models Emotion Sentiment Naive Bayes 52.70 74.89 SVM 55.08 76.04 Logistic Regression 56.18 74.89 IndoNLU IndoBERT Base 54.50 75.24 IndoNLU IndoBERT Large 57.80 77.40 IndoLEM IndoBERT Base 52.59 69.08 mBERT 44.13 68.72 XLM-R Base 47.02 68.62 XLM-R Large 54.84 79.06 Our work LazarusNLP/NusaBERT-base 56.54 77.07 LazarusNLP/NusaBERT-large 61.40 79.54"},{"location":"projects/nusabert/#installation","title":"Installation","text":"<pre><code>git clone https://github.com/LazarusNLP/NusaBERT.git\ncd NusaBERT\npip install -r requirements.txt\n</code></pre>"},{"location":"projects/nusabert/#dataset","title":"Dataset","text":"<p>For pre-training we leverage three existing open-source corpora that includes the Indonesian language and regional languages of Indonesia. A summary of the datasets are as follows:</p> Dataset Language #documents uonlp/CulturaX Indonesian (<code>ind</code>) 23,251,368 uonlp/CulturaX Javanese (<code>jav</code>) 2,058 uonlp/CulturaX Malay (<code>msa</code>) 238,000 uonlp/CulturaX Sundanese (<code>sun</code>) 1,554 sabilmakbar/indo_wiki Acehnese (<code>ace</code>) 12,904 sabilmakbar/indo_wiki Balinese (<code>ban</code>) 19,837 sabilmakbar/indo_wiki Banjarese (<code>bjn</code>) 10,437 sabilmakbar/indo_wiki Buginese (<code>bug</code>) 9,793 sabilmakbar/indo_wiki Gorontalo (<code>gor</code>) 14,514 sabilmakbar/indo_wiki Indonesian (<code>ind</code>) 654,287 sabilmakbar/indo_wiki Javanese (<code>jav</code>) 72,667 sabilmakbar/indo_wiki Banyumasan (<code>map_bms</code>) 11,832 sabilmakbar/indo_wiki Minangkabau (<code>min</code>) 225,858 sabilmakbar/indo_wiki Malay (<code>msa</code>) 346,186 sabilmakbar/indo_wiki Nias (<code>nia</code>) 1,650 sabilmakbar/indo_wiki Sundanese (<code>sun</code>) 61,494 sabilmakbar/indo_wiki Tetum (<code>tet</code>) 1,465 acul3/KoPI-NLLB Acehnese (<code>ace</code>) 792,594 acul3/KoPI-NLLB Balinese (<code>ban</code>) 244,545 acul3/KoPI-NLLB Banjarese (<code>bjn</code>) 296,314 acul3/KoPI-NLLB Javanese (<code>jav</code>) 1,155,142 acul3/KoPI-NLLB Minangkabau (<code>min</code>) 113,323 acul3/KoPI-NLLB Sundanese (<code>sun</code>) 894,626"},{"location":"projects/nusabert/#extend-nusabert-tokenizer","title":"Extend NusaBERT Tokenizer","text":"<p>We first need to train a WordPiece tokenizer on our pre-pretraining corpus, whose vocab size we limit up to 10,000. We then add non-overlapping tokens from the new tokenizer to the original IndoBERT tokenizer. Since there are overlapping tokens between the two tokenizers, we only ended up adding 1,511 new tokens to the original tokenizer. Refer to the script for more details.</p>"},{"location":"projects/nusabert/#pre-train-nusabert","title":"Pre-train NusaBERT","text":"<p>We modified the Hugging Face \ud83e\udd17 masked language modeling pre-training script and conducted continued pre-training of IndoBERT on the dataset detailed above. Running pre-training is as simple as:</p> <pre><code>python scripts/run_mlm.py \\\n    --model_name_or_path indobenchmark/indobert-base-p1 \\\n    --tokenizer_name LazarusNLP/nusabert-base \\\n    --max_seq_length 128 \\\n    --per_device_train_batch_size 256 \\\n    --per_device_eval_batch_size 256 \\\n    --do_train --do_eval \\\n    --max_steps 500000 \\\n    --warmup_steps 24000 \\\n    --learning_rate 3e-4 \\\n    --weight_decay 0.01 \\\n    --optim adamw_torch_fused \\\n    --bf16 \\\n    --preprocessing_num_workers 24 \\\n    --dataloader_num_workers 24 \\\n    --save_steps 10000 --save_total_limit 3 \\\n    --output_dir outputs/nusabert-base \\\n    --overwrite_output_dir \\\n    --report_to tensorboard \\\n    --push_to_hub --hub_private_repo \\\n    --hub_model_id LazarusNLP/nusabert-base\n</code></pre> <p>We achieved a negative log-likelihood loss of 1.4876 and an accuracy of 68.66% on a heldout subset (5%) of the pre-training corpus.</p>"},{"location":"projects/nusabert/#fine-tune-nusabert","title":"Fine-tune NusaBERT","text":"<p>We developed fine-tuning scripts for NusaBERT based on fine-tuning scripts from Hugging Face \ud83e\udd17's sample fine-tuning scripts.</p> <p>In particular, we developed fine-tuning scripts for single-sentence classification, multi-class multi-label classification, token classification, and pair token classification, which you can find in scripts. These scripts support IndoNLU, NusaX, and NusaWrites datasets.</p>"},{"location":"projects/nusabert/#single-sentence-classification-task","title":"Single-Sentence Classification Task","text":"<p>The tasks included under this category are emotion classification, sentiment analysis, topic classification, etc. To fine-tune for single-sentence classification, run the following command and modify accordingly:</p> <pre><code>python scripts/run_classification.py \\\n    --model-checkpoint LazarusNLP/NusaBERT-base \\\n    --dataset-name indonlp/indonlu \\\n    --dataset-config emot \\\n    --input-column-names tweet \\\n    --target-column-name label \\\n    --input-max-length 128 \\\n    --output-dir outputs/nusabert-base-emot \\\n    --num-train-epochs 100 \\\n    --optim adamw_torch_fused \\\n    --learning-rate 1e-5 \\\n    --weight-decay 0.01 \\\n    --per-device-train-batch-size 32 \\\n    --per-device-eval-batch-size 64 \\\n    --hub-model-id LazarusNLP/NusaBERT-base-EmoT\n</code></pre> <p>Single-Sentence Classification recipes are provided here.</p>"},{"location":"projects/nusabert/#multi-label-multi-class-classification","title":"Multi-label Multi-class Classification","text":"<p>The task included under this category is aspect-based sentiment analysis (e.g. IndoNLU CASA and HoASA). To fine-tune for multi-label multi-class classification, run the following command and modify accordingly:</p> <pre><code>python scripts/run_multi_label_classification.py \\\n    --model-checkpoint LazarusNLP/NusaBERT-base \\\n    --dataset-name indonlp/indonlu \\\n    --dataset-config casa \\\n    --input-column-name sentence \\\n    --target-column-names fuel,machine,others,part,price,service \\\n    --input-max-length 128 \\\n    --output-dir outputs/nusabert-base-casa \\\n    --num-train-epochs 100 \\\n    --optim adamw_torch_fused \\\n    --learning-rate 1e-5 \\\n    --weight-decay 0.01 \\\n    --per-device-train-batch-size 32 \\\n    --per-device-eval-batch-size 64 \\\n    --hub-model-id LazarusNLP/NusaBERT-base-CASA\n</code></pre> <p>Multi-label Multi-class Classification recipes are provided here.</p>"},{"location":"projects/nusabert/#token-classification","title":"Token Classification","text":"<p>Token classification is also known as sequence labeling. The tasks included under this category are part-of-speech tagging (POS), named entity recognition (NER), and token-level span extraction (e.g. IndoNLU TermA, KEPS). To fine-tune for token classification, run the following command and modify accordingly:</p> <pre><code>python scripts/run_token_classification.py \\\n        --model-checkpoint LazarusNLP/NusaBERT-base \\\n        --dataset-name indonlp/indonlu \\\n        --dataset-config posp \\\n        --input-column-name tokens \\\n        --target-column-name pos_tags \\\n        --output-dir outputs/nusabert-base-posp \\\n        --num-train-epochs 10 \\\n        --optim adamw_torch_fused \\\n        --learning-rate 2e-5 \\\n        --weight-decay 0.01 \\\n        --per-device-train-batch-size 16 \\\n        --per-device-eval-batch-size 64 \\\n        --hub-model-id LazarusNLP/NusaBERT-base-POSP\n</code></pre> <p>Token Classification recipes are provided here.</p>"},{"location":"projects/nusabert/#pair-token-classification","title":"Pair Token Classification","text":"<p>Pair token classification is much like token-classification, except involving a pair of input sentences instead of one. The tasks included under this category is token-level question-passage-answering (e.g. IndoNLU FacQA). To fine-tune for pair question-answering, run the following command and modify accordingly:</p> <pre><code>python scripts/run_pair_token_classification.py \\\n    --model-checkpoint LazarusNLP/NusaBERT-base \\\n    --dataset-name indonlp/indonlu \\\n    --dataset-config facqa \\\n    --input-column-name-1 question \\\n    --input-column-name-2 passage \\\n    --target-column-name seq_label \\\n    --output-dir outputs/nusabert-base-facqa \\\n    --num-train-epochs 10 \\\n    --optim adamw_torch_fused \\\n    --learning-rate 2e-5 \\\n    --weight-decay 0.01 \\\n    --per-device-train-batch-size 16 \\\n    --per-device-eval-batch-size 64 \\\n    --hub-model-id LazarusNLP/NusaBERT-base-FacQA\n</code></pre> <p>Pair Token Classification recipes are provided here.</p>"},{"location":"projects/nusabert/#credits","title":"Credits","text":"<p>NusaBERT is developed with love by:</p>"},{"location":"projects/sentence-embeddings/","title":"Sentence Embeddings","text":"<ul> <li> GitHub Repository</li> <li> Documentation</li> <li>\ud83e\udd17 HuggingFace Collection</li> </ul> <p>Inspired by Thai Sentence Vector Benchmark, we decided to embark on the journey of training Indonesian sentence embedding models!</p> <p> </p>"},{"location":"projects/sentence-embeddings/#evaluation","title":"Evaluation","text":""},{"location":"projects/sentence-embeddings/#semantic-textual-similarity","title":"Semantic Textual Similarity","text":"<p>We believe that a synthetic baseline is better than no baseline. Therefore, we followed approached done in the Thai Sentence Vector Benchmark project and translated the STS-B dev and test set to Indonesian via Google Translate API. This dataset will be used to evaluate our model's Spearman correlation score on the translated test set.</p> <p>You can find the translated dataset on \ud83e\udd17 HuggingFace Hub.</p>"},{"location":"projects/sentence-embeddings/#retrieval","title":"Retrieval","text":"<p>To evaluate our models' capability to perform retrieval tasks, we evaluate them on Indonesian subsets of MIRACL and TyDiQA datasets. In both datasets, the model's ability to retrieve relevant documents given a query is tested. We employ R@1 (top-1 accuracy), MRR@10, and nDCG@10 metrics to measure our model's performance.</p>"},{"location":"projects/sentence-embeddings/#classification","title":"Classification","text":"<p>For text classification, we will be doing emotion classification and sentiment analysis on the EmoT and SmSA subsets of IndoNLU, respectively. To do so, we will be doing the same approach as Thai Sentence Vector Benchmark and simply fit a Linear SVC on sentence representations of our texts with their corresponding labels. Thus, unlike conventional fine-tuning method where the backbone model is also updated, the Sentence Transformer stays frozen in our case; with only the classification head being trained.</p> <p>Further, we will evaluate our models using the official MTEB code that contains two Indonesian classification subtasks: <code>MassiveIntentClassification (id)</code> and <code>MassiveScenarioClassification (id)</code>.</p>"},{"location":"projects/sentence-embeddings/#pair-classification","title":"Pair Classification","text":"<p>We followed MTEB's PairClassification evaluation procedure for pair classification. Specifically for zero-shot natural language inference tasks, all neutral pairs are dropped, while contradictions and entailments are re-mapped as <code>0</code>s and <code>1</code>s. The maximum average precision (AP) score is found by finding the best threshold value.</p> <p>We leverage the IndoNLI dataset's two test subsets: <code>test_lay</code> and <code>test_expert</code>.</p>"},{"location":"projects/sentence-embeddings/#methods","title":"Methods","text":""},{"location":"projects/sentence-embeddings/#unsupervised-simcse","title":"(Unsupervised) SimCSE","text":"<p>We followed SimCSE: Simple Contrastive Learning of Sentence Embeddings and trained a sentence embedding model in an unsupervised fashion. Unsupervised SimCSE allows us to leverage an unsupervised corpus -- which are plenty -- and with different dropout masks in the encoder, contrastively learn sentence representations. This is parallel with the situation that there is a lack of supervised Indonesian sentence similarity datasets, hence SimCSE is a natural first move into this field. We used the Sentence Transformer implementation of SimCSE.</p>"},{"location":"projects/sentence-embeddings/#congen","title":"ConGen","text":"<p>Like SimCSE, ConGen: Unsupervised Control and Generalization Distillation For Sentence Representation is another unsupervised technique to train a sentence embedding model. Since it is in-part a distillation method, ConGen relies on a teacher model which will then be distilled to a student model. The original paper proposes back-translation as the best data augmentation technique. However, due to the lack of resources, we implemented word deletion, which was found to be on-par with back-translation despite being trivial. We used the official ConGen implementation which was written on top of the Sentence Transformers library.</p>"},{"location":"projects/sentence-embeddings/#sct","title":"SCT","text":"<p>SCT: An Efficient Self-Supervised Cross-View Training For Sentence Embedding is another unsupervised technique to train a sentence embedding model. It is very similar to ConGen in its knowledge distillation methodology, but also supports self-supervised training procedure without a teacher model. The original paper proposes back-translation as its data augmentation technique, but we implemented single-word deletion and found it to perform better than our backtranslated corpus. We used the official SCT implementation which was written on top of the Sentence Transformers library.</p>"},{"location":"projects/sentence-embeddings/#pretrained-models","title":"Pretrained Models","text":"Model #params Base/Student Model Teacher Model Train Dataset Supervised SimCSE-IndoBERT Base 125M IndoBERT Base N/A Wikipedia ConGen-IndoBERT Lite Base 12M IndoBERT Lite Base paraphrase-multilingual-mpnet-base-v2 Wikipedia ConGen-IndoBERT Base 125M IndoBERT Base paraphrase-multilingual-mpnet-base-v2 Wikipedia ConGen-SimCSE-IndoBERT Base 125M SimCSE-IndoBERT Base paraphrase-multilingual-mpnet-base-v2 Wikipedia ConGen-Indo-e5 Small 118M multilingual-e5-small paraphrase-multilingual-mpnet-base-v2 Wikipedia SCT-IndoBERT Base 125M IndoBERT Base paraphrase-multilingual-mpnet-base-v2 Wikipedia all-IndoBERT Base 125M IndoBERT Base N/A See: README \u2705 all-IndoBERT Base-v2 125M IndoBERT Base N/A See: README \u2705 all-Indo-e5 Small-v2 118M multilingual-e5-small N/A See: README \u2705 all-Indo-e5 Small-v3 118M multilingual-e5-small N/A See: README \u2705 distiluse-base-multilingual-cased-v2 134M DistilBERT Base Multilingual mUSE See: SBERT \u2705 paraphrase-multilingual-mpnet-base-v2 125M XLM-RoBERTa Base paraphrase-mpnet-base-v2 See: SBERT \u2705 multilingual-e5-small 118M Multilingual-MiniLM-L12-H384 See: arXiv See: \ud83e\udd17 \u2705 multilingual-e5-base 278M XLM-RoBERTa Base See: arXiv See: \ud83e\udd17 \u2705 multilingual-e5-large 560M XLM-RoBERTa Large See: arXiv See: \ud83e\udd17 \u2705 Deprecated Models Model #params Base/Student Model Teacher Model Train Dataset Supervised SimCSE-IndoBERT Lite Base 12M IndoBERT Lite Base N/A Wikipedia SimCSE-IndoRoBERTa Base 125M IndoRoBERTa Base N/A Wikipedia S-IndoBERT Base mMARCO 125M IndoBERT Base N/A mMARCO \u2705 all-IndoBERT Base p2 125M IndoBERT Base p2 N/A See: README \u2705"},{"location":"projects/sentence-embeddings/#results","title":"Results","text":""},{"location":"projects/sentence-embeddings/#semantic-textual-similarity_1","title":"Semantic Textual Similarity","text":""},{"location":"projects/sentence-embeddings/#machine-translated-indonesian-sts-b","title":"Machine Translated Indonesian STS-B","text":"Model Spearman's Correlation (%) \u2191 SimCSE-IndoBERT Base 70.13 ConGen-IndoBERT Lite Base 79.97 ConGen-IndoBERT Base 80.47 ConGen-SimCSE-IndoBERT Base 81.16 ConGen-Indo-e5 Small 80.94 SCT-IndoBERT Base 74.56 all-IndoBERT Base 73.84 all-IndoBERT Base-v2 76.03 all-Indo-e5 Small-v2 79.57 all-Indo-e5 Small-v3 79.95 distiluse-base-multilingual-cased-v2 75.08 paraphrase-multilingual-mpnet-base-v2 83.83 multilingual-e5-small 78.89 multilingual-e5-base 79.72 multilingual-e5-large 79.44"},{"location":"projects/sentence-embeddings/#retrieval_1","title":"Retrieval","text":""},{"location":"projects/sentence-embeddings/#miracl","title":"MIRACL","text":"Model R@1 (%) \u2191 MRR@10 (%) \u2191 nDCG@10 (%) \u2191 SimCSE-IndoBERT Base 36.04 48.25 39.70 ConGen-IndoBERT Lite Base 46.04 59.06 51.01 ConGen-IndoBERT Base 45.93 58.58 49.95 ConGen-SimCSE-IndoBERT Base 45.83 58.27 49.91 ConGen-Indo-e5 Small 55.00 66.74 58.95 SCT-IndoBERT Base 40.41 47.29 40.68 all-IndoBERT Base 65.52 75.92 70.13 all-IndoBERT Base-v2 67.18 76.59 70.16 all-Indo-e5 Small-v2 68.33 78.33 73.04 all-Indo-e5 Small-v3 68.12 78.22 73.09 distiluse-base-multilingual-cased-v2 41.35 54.93 48.79 paraphrase-multilingual-mpnet-base-v2 52.81 65.07 57.97 multilingual-e5-small 70.20 79.61 74.80 multilingual-e5-base 70.00 79.50 75.16 multilingual-e5-large 70.83 80.58 76.16"},{"location":"projects/sentence-embeddings/#tydiqa","title":"TyDiQA","text":"Model R@1 (%) \u2191 MRR@10 (%) \u2191 nDCG@10 (%) \u2191 SimCSE-IndoBERT Base 61.94 69.89 73.52 ConGen-IndoBERT Lite Base 75.22 81.55 84.13 ConGen-IndoBERT Base 73.09 80.32 83.29 ConGen-SimCSE-IndoBERT Base 72.38 79.37 82.51 ConGen-Indo-e5 Small 84.60 89.30 91.27 SCT-IndoBERT Base 76.81 83.16 85.87 all-IndoBERT Base 88.14 91.47 92.91 all-IndoBERT Base-v2 87.61 90.91 92.31 all-Indo-e5 Small-v2 93.27 95.63 96.46 all-Indo-e5 Small-v3 93.27 95.72 96.58 distiluse-base-multilingual-cased-v2 70.44 77.94 81.56 paraphrase-multilingual-mpnet-base-v2 81.41 87.05 89.44 multilingual-e5-small 91.50 94.34 95.39 multilingual-e5-base 93.45 95.88 96.69 multilingual-e5-large 94.69 96.71 97.44"},{"location":"projects/sentence-embeddings/#classification_1","title":"Classification","text":""},{"location":"projects/sentence-embeddings/#mteb-massive-intent-classification-id","title":"MTEB - Massive Intent Classification <code>(id)</code>","text":"Model Accuracy (%) \u2191 F1 Macro (%) \u2191 SimCSE-IndoBERT Base 59.71 57.70 ConGen-IndoBERT Lite Base 62.41 60.94 ConGen-IndoBERT Base 61.14 60.02 ConGen-SimCSE-IndoBERT Base 60.93 59.50 ConGen-Indo-e5 Small 62.92 60.18 SCT-IndoBERT Base 55.66 54.48 all-IndoBERT Base 58.40 57.21 all-IndoBERT Base-v2 58.31 57.11 all-Indo-e5 Small-v2 61.51 59.24 all-Indo-e5 Small-v3 61.63 59.29 distiluse-base-multilingual-cased-v2 55.99 52.44 paraphrase-multilingual-mpnet-base-v2 65.43 63.55 multilingual-e5-small 64.16 61.33 multilingual-e5-base 66.63 63.88 multilingual-e5-large 70.04 67.66"},{"location":"projects/sentence-embeddings/#mteb-massive-scenario-classification-id","title":"MTEB - Massive Scenario Classification <code>(id)</code>","text":"Model Accuracy (%) \u2191 F1 Macro (%) \u2191 SimCSE-IndoBERT Base 66.14 65.56 ConGen-IndoBERT Lite Base 67.25 66.53 ConGen-IndoBERT Base 67.72 67.32 ConGen-SimCSE-IndoBERT Base 67.12 66.64 ConGen-Indo-e5 Small 66.92 66.29 SCT-IndoBERT Base 61.89 60.97 all-IndoBERT Base 66.37 66.31 all-IndoBERT Base-v2 66.02 65.97 all-Indo-e5 Small-v2 67.02 66.86 all-Indo-e5 Small-v3 67.27 67.13 distiluse-base-multilingual-cased-v2 65.25 63.45 paraphrase-multilingual-mpnet-base-v2 70.72 70.58 multilingual-e5-small 67.92 67.23 multilingual-e5-base 70.70 70.26 multilingual-e5-large 74.11 73.82"},{"location":"projects/sentence-embeddings/#indonlu-emotion-classification-emot","title":"IndoNLU - Emotion Classification (EmoT)","text":"Model Accuracy (%) \u2191 F1 Macro (%) \u2191 SimCSE-IndoBERT Base 55.45 55.78 ConGen-IndoBERT Lite Base 58.18 58.84 ConGen-IndoBERT Base 57.04 57.06 ConGen-SimCSE-IndoBERT Base 59.54 60.37 ConGen-Indo-e5 Small 60.00 60.52 SCT-IndoBERT Base 61.13 61.70 all-IndoBERT Base 57.27 57.47 all-IndoBERT Base-v2 58.86 59.31 all-Indo-e5 Small-v2 58.18 57.99 all-Indo-e5 Small-v3 56.81 56.46 distiluse-base-multilingual-cased-v2 63.63 64.13 paraphrase-multilingual-mpnet-base-v2 63.18 63.78 multilingual-e5-small 64.54 65.04 multilingual-e5-base 68.63 69.07 multilingual-e5-large 74.77 74.66"},{"location":"projects/sentence-embeddings/#indonlu-sentiment-analysis-smsa","title":"IndoNLU - Sentiment Analysis (SmSA)","text":"Model Accuracy (%) \u2191 F1 Macro (%) \u2191 SimCSE-IndoBERT Base 85.6 81.50 ConGen-IndoBERT Lite Base 81.2 75.59 ConGen-IndoBERT Base 85.4 82.12 ConGen-SimCSE-IndoBERT Base 83.0 78.74 ConGen-Indo-e5 Small 84.2 80.21 SCT-IndoBERT Base 82.0 76.92 all-IndoBERT Base 84.4 79.79 all-IndoBERT Base-v2 83.4 79.04 all-Indo-e5 Small-v2 82.0 78.15 all-Indo-e5 Small-v3 82.6 78.98 distiluse-base-multilingual-cased-v2 78.8 73.64 paraphrase-multilingual-mpnet-base-v2 89.6 86.56 multilingual-e5-small 83.6 79.51 multilingual-e5-base 89.4 86.22 multilingual-e5-large 90.0 86.50"},{"location":"projects/sentence-embeddings/#pair-classification_1","title":"Pair Classification","text":""},{"location":"projects/sentence-embeddings/#indonli","title":"IndoNLI","text":"Model <code>test_lay</code> AP (%) \u2191 <code>test_expert</code> AP (%) \u2191 SimCSE-IndoBERT Base 56.06 50.72 ConGen-IndoBERT Lite Base 69.44 53.74 ConGen-IndoBERT Base 71.14 56.35 ConGen-SimCSE-IndoBERT Base 70.80 56.59 ConGen-Indo-e5 Small 70.51 55.67 SCT-IndoBERT Base 59.82 53.41 all-IndoBERT Base 72.01 56.79 all-IndoBERT Base-v2 71.36 56.83 all-Indo-e5 Small-v2 76.29 57.05 all-Indo-e5 Small-v3 75.21 56.62 distiluse-base-multilingual-cased-v2 58.48 50.50 paraphrase-multilingual-mpnet-base-v2 74.87 57.96 multilingual-e5-small 63.97 51.85 multilingual-e5-base 60.25 50.91 multilingual-e5-large 61.39 51.62"},{"location":"projects/sentence-embeddings/#credits","title":"Credits","text":"<p>Indonesian Sentence Embeddings is developed with love by:</p>"},{"location":"projects/t5-language-models/","title":"Indonesian T5 Language Models","text":"<ul> <li> GitHub Repository</li> <li>\ud83e\udd17 HuggingFace Collection</li> </ul> <p>This project focuses on pre-training a T5 (Text-to-Text Transfer Transformer) model specifically for the Indonesian language, using nanoT5 as its training framework. Our aim is to provide fully open-source, budget-constrained, sequence-to-sequence language models for Indonesia that are on-par with state-of-the-art models!</p> <p> </p>"},{"location":"projects/t5-language-models/#pre-trained-models","title":"Pre-trained Models","text":"Model #params Dataset LazarusNLP/IndoNanoT5-base 248M uonlp/CulturaX"},{"location":"projects/t5-language-models/#results","title":"Results","text":"<p>We evaluate our models on IndoNLG, which consists of multiple downsteam generation tasks in Indonesian. The dataset also supports Javanese and Sundanese, but as our model is currently monolingual, we fine-tune on Indonesian tasks only.</p> <p>IndoNLG baseline results are obtained from the official IndoNLG paper.</p>"},{"location":"projects/t5-language-models/#indosum","title":"IndoSum","text":"Model #params R1 \u2191 R2 \u2191 RL \u2191 Scratch 132M 70.52 65.43 68.35 mBART Large 610M 74.65 70.43 72.54 mT5 Small 300M 74.04 69.64 71.89 IndoBART 132M 70.67 65.59 68.18 IndoGPT 117M 74.49 70.34 72.46 Our work LazarusNLP/IndoNanoT5-base 248M 75.29 71.23 73.30"},{"location":"projects/t5-language-models/#liputan6-canonical","title":"Liputan6 Canonical","text":"Model #params R1 \u2191 R2 \u2191 RL \u2191 Scratch 132M 38.14 20.67 31.85 See et al. (2017) 22M 36.09 19.19 29.81 Koto et al. (2020) 153M 41.06 22.83 34.23 mBART Large 610M 39.17 21.75 32.85 mT5 Small 300M 39.69 22.03 33.28 IndoBART 132M 39.87 22.24 33.50 IndoGPT 117M 37.41 20.61 31.54 Our work LazarusNLP/IndoNanoT5-base 248M 39.76 22.29 33.46"},{"location":"projects/t5-language-models/#liputan6-extreme","title":"Liputan6 Extreme","text":"Model #params R1 \u2191 R2 \u2191 RL \u2191 Scratch 132M 32.47 13.45 25.52 See et al. (2017) 22M 30.39 12.03 23.55 Koto et al. (2020) 153M 34.84 15.03 27.44 mBART Large 610M 32.87 13.79 25.91 mT5 Small 300M 33.37 14.01 26.21 IndoBART 132M 33.58 14.45 26.68 IndoGPT 117M 31.45 13.09 24.91 Our work LazarusNLP/IndoNanoT5-base 248M 33.23 14.17 26.21"},{"location":"projects/t5-language-models/#tydiqa","title":"TyDiQA","text":"Model #params EM \u2191 F1 \u2191 Scratch 132M 21.40 29.77 mBART Large 610M 62.69 76.41 mT5 Small 300M 35.67 51.90 IndoBART 132M 57.31 69.59 IndoGPT 117M 50.18 63.97 Our work LazarusNLP/IndoNanoT5-base 248M 58.94 72.19"},{"location":"projects/t5-language-models/#xpersona","title":"XPersona","text":"Model #params SacreBLEU \u2191 BLEU \u2191 Scratch 132M 1.86 1.86 CausalBERT \\(^\\dagger\\) 110M 2.24 2.23 mBART Large 610M 2.57 2.56 mT5 Small 300M 1.90 1.89 IndoBART 132M 2.93 2.93 IndoGPT 117M 2.02 2.02 Our work \\(^\\dagger\\) LazarusNLP/IndoNanoT5-base 248M 4.07 4.07 <p>\\(^\\dagger\\) Our models are trained with additional persona information, just like the original CausalBERT baseline. The remaining models are not trained with persona information. Our findings suggest that persona information is crucial for this task; serving a similar purpose to system prompts in recent LLM development.</p>"},{"location":"projects/t5-language-models/#installation","title":"Installation","text":"<pre><code>git clone https://github.com/LazarusNLP/IndoT5.git\ncd IndoT5\ngit submodule update --init # clone nanoT5 submodule\npip install -r requirements.txt\npip install -r nanoT5/requirements.txt\n</code></pre>"},{"location":"projects/t5-language-models/#dataset","title":"Dataset","text":"<p>We leverage the existing uonlp/CulturaX dataset which contains 23M Indonesian documents, collected and cleaned from the OSCAR corpora and mc4. We selected this dataset as it is sufficiently large and has been deduplicated. More details can be found in their dataset card.</p> <p>Since this dataset is rather large, we utilize the dataset streaming feature of Hugging Face datasets, which is thankfully also supported in nanoT5. This feature is likewise usable during tokenizer training.</p>"},{"location":"projects/t5-language-models/#train-sentencepiece-tokenizer","title":"Train SentencePiece Tokenizer","text":"<p>We first need to train a SentencePiece tokenizer on our pre-pretraining corpus. We followed the uncased T5 tokenizer training implementation from HuggingFace. We then initialize a T5 config based on google/t5-v1_1-base and the newly trained tokenizer. Both the tokenizer and the config are then saved for loading later. </p> <p>To train the SentencePiece tokenizer, run <code>train_tokenizer.py</code> with the desired arguments:</p> <pre><code>python train_tokenizer.py \\\n    --vocab-size 32000 \\\n    --dataset-name uonlp/CulturaX \\\n    --dataset-config id \\\n    --output-dir outputs/indonesian-t5-base/ \\\n    --base-model-config google/t5-v1_1-base \\\n    --hf-repo-id LazarusNLP/IndoNanoT5-base\n</code></pre> <p>It took us about an hour to train the tokenizer.</p>"},{"location":"projects/t5-language-models/#pre-train-t5","title":"Pre-train T5","text":"<p>NanoT5 handles most of the training process and exposes a clean API to pre-train a T5 model from scratch. We follow their default training configuration, with the exception of a lower learning rate which is specific to our dataset. Other than that, running pre-training is as simple as:</p> <pre><code>python -m nanoT5.main \\\n    optim.name=adamwscale \\\n    optim.lr_scheduler=cosine \\\n    optim.base_lr=5e-3 \\\n    model.name=LazarusNLP/IndoNanoT5-base \\\n    model.compile=true \\\n    data.num_workers=16\n</code></pre> <p>We achieved a negative log-likelihood loss of 2.082 and an accuracy of 57.4% on a heldout subset (1%) of the pre-training corpus.</p>"},{"location":"projects/t5-language-models/#experiments","title":"Experiments","text":"<p>We experimented with different learning rates, optimizers, and layer initialization strategies. Whilst we found that the default scaled AdamW optimizer worked best for our baseline results, we aim to further improve the results. Specifically, we aim to experiment with:</p> <ul> <li> Initializing <code>lm_head</code> weights with <code>std=1/sqrt(d_model)</code></li> <li> (Unscaled) AdamW Optimizer</li> <li> NAdamW Optimizer</li> <li> Shampoo and CASPR Optimizers</li> </ul> <p>This growing list of ideas stem from a fruitful discussion here.</p> Training Losses"},{"location":"projects/t5-language-models/#fine-tune-t5","title":"Fine-tune T5","text":"<p>NanoT5 supports fine-tuning to a downstream dataset like Super Natural-Instructions (SNI). However, since this requires further customization of fine-tuning code to other downstream datasets, we opted to develop our own fine-tuning script based on Hugging Face's sample fine-tuning code.</p> <p>In particular, we developed fine-tuning scripts for 3 IndoNLG tasks, namely: summarization, question-answering, and chit-chat (conversational), which you can find in scripts.</p>"},{"location":"projects/t5-language-models/#summarization","title":"Summarization","text":"<p>To fine-tune for summarization, run the following command and modify accordingly:</p> <pre><code>python scripts/run_summarization.py \\\n    --model-checkpoint LazarusNLP/IndoNanoT5-base \\ # pre-trained model checkpoint\n    --dataset-name LazarusNLP/indonlg \\ # Hugging Face \ud83e\udd17 dataset name\n    --dataset-config indosum \\ # dataset config\n    --input-column-name input \\ # input column (text passage) name in dataset\n    --target-column-name target \\ # target column (summary) name in dataset\n    --input-max-length 512 \\\n    --target-max-length 512 \\\n    --num-beams 5 \\ # beam width during beam search\n    --output-dir outputs/indo-nanot5-indosum \\\n    --num-train-epochs 5 \\\n    --optim adamw_torch_fused \\ # any optimizer supported in Hugging Face \ud83e\udd17 transformers\n    --learning-rate 1e-3 \\\n    --weight-decay 0.01 \\\n    --per-device-train-batch-size 8 \\\n    --per-device-eval-batch-size 16 \\\n    --hub-model-id LazarusNLP/IndoNanoT5-base-IndoSum # Hugging Face \ud83e\udd17 Hub repo name\n</code></pre> <p>IndoNLG summarization recipes are provided here.</p>"},{"location":"projects/t5-language-models/#question-answering","title":"Question-Answering","text":"<p>To fine-tune for question-answering, run the following command and modify accordingly:</p> <pre><code>python scripts/run_qa.py \\\n    --model-checkpoint LazarusNLP/IndoNanoT5-base \\\n    --dataset-name LazarusNLP/indonlg \\\n    --dataset-config question_answering \\\n    --context-column-name context \\ # context/passage column name\n    --question-column-name input \\ # question column name\n    --answer-column-name references \\ # answer column name, must be list\n    --id-column-name gem_id \\ # question-answer pair id\n    --input-max-length 512 \\\n    --target-max-length 512 \\\n    --num-beams 5 \\\n    --output-dir outputs/indo-nanot5-tydiqa \\\n    --num-train-epochs 50 \\\n    --optim adamw_torch_fused \\\n    --learning-rate 1e-5 \\\n    --weight-decay 0.01 \\\n    --per-device-train-batch-size 8 \\\n    --per-device-eval-batch-size 16 \\\n    --hub-model-id LazarusNLP/IndoNanoT5-base-TyDiQA\n</code></pre> <p>IndoNLG question-answering recipe is provided here.</p>"},{"location":"projects/t5-language-models/#chit-chat","title":"Chit-chat","text":"<p>To fine-tune for chit-chat, run the following command and modify accordingly:</p> <pre><code>python scripts/run_chitchat.py \\\n    --model-checkpoint LazarusNLP/IndoNanoT5-base \\\n    --dataset-name LazarusNLP/indonlg \\\n    --dataset-config xpersona \\\n    --context-column-name context \\ # context/persona column name\n    --question-column-name input \\ # conversation history/dialogues column name\n    --answer-column-name references \\ # response column name\n    --use-persona \\ # whether to use persona or not\n    --input-max-length 512 \\\n    --target-max-length 512 \\\n    --num-beams 5 \\\n    --output-dir outputs/indo-nanot5-xpersona \\\n    --num-train-epochs 50 \\\n    --optim adamw_torch_fused \\\n    --learning-rate 1e-5 \\\n    --weight-decay 0.01 \\\n    --per-device-train-batch-size 8 \\\n    --per-device-eval-batch-size 16 \\\n    --hub-model-id LazarusNLP/IndoNanoT5-base-XPersona\n</code></pre>"},{"location":"projects/t5-language-models/#acknowledgements","title":"Acknowledgements","text":"<p>Thanks to @PiotrNawrot and @Birch-san for the engaging discussion and ideas.</p>"},{"location":"projects/t5-language-models/#references","title":"References","text":"<pre><code>@article{Nawrot2023nanoT5AP,\n  title={nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources},\n  author={Piotr Nawrot},\n  journal={ArXiv},\n  year={2023},\n  volume={abs/2309.02373},\n}\n</code></pre>"},{"location":"projects/t5-language-models/#credits","title":"Credits","text":"<p>IndoT5 is developed with love by:</p>"}]}